{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ee158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Change to correct dataset directory\n",
    "root_directory = '/Users/divya/Documents/Data_collection/final_correct_dataset_with_default//'\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Walk through the root directory and its subdirectories\n",
    "for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one big DataFrame\n",
    "big_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "big_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "# Define min-max values for each column (modify these as needed)\n",
    "column_min_max_values = {\n",
    "    'angle': (big_dataframe['angle'].min(), big_dataframe['angle'].max()),\n",
    "    'fsr1': (big_dataframe['fsr1'].min(), big_dataframe['fsr1'].max()),\n",
    "    'fsr2': (big_dataframe['fsr2'].min(), big_dataframe['fsr2'].max()),\n",
    "    'fsr3': (big_dataframe['fsr3'].min(), big_dataframe['fsr3'].max()),\n",
    "    'fsr4': (big_dataframe['fsr4'].min(), big_dataframe['fsr4'].max()),\n",
    "    'acc2x':(-15.0,15.0),\n",
    "    'acc2y':(-15.0,15.0),\n",
    "    'acc2z':(-15.0,15.0),\n",
    "    'gyro1':(-6.0,6.0),\n",
    "    'gyro2':(-6.0,6.0),\n",
    "    'gyro3':(-6.0,6.0),\n",
    "    #'orient1': (0, 350),\n",
    "    #'orient2': (0, 100),\n",
    "    #'orient3': (0, 100),\n",
    "}\n",
    "\n",
    "root_directory = '/Users/divya/Documents/Data_collection/final_correct_dataset_with_default//'\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Walk through the root directory and its subdirectories\n",
    "for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            normalized_data = df.copy()\n",
    "            for column, (min_val, max_val) in column_min_max_values.items():\n",
    "                normalized_data[column] = (df[column] - min_val) / (max_val - min_val)\n",
    "            dataframes.append(normalized_data)\n",
    "\n",
    "# Concatenate all DataFrames into one big DataFrame\n",
    "big_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "big_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is to select the 2 out of 4 FSRs for all the participants\n",
    "# Group by 'participant'\n",
    "grouped_df = big_dataframe.groupby('participant')\n",
    "\n",
    "# Mapping of participant to the columns to copy\n",
    "participant_column_mapping = {\n",
    "    1: ['fsr2', 'fsr4'],\n",
    "    2: ['fsr2', 'fsr4'],\n",
    "    3: ['fsr2', 'fsr4'],\n",
    "    4: ['fsr2', 'fsr4'],\n",
    "    5: ['fsr2', 'fsr4'],\n",
    "    6: ['fsr2', 'fsr4'],\n",
    "    9: ['fsr2', 'fsr4'],\n",
    "    10: ['fsr2', 'fsr3'],\n",
    "    11: ['fsr2', 'fsr4'],\n",
    "    12: ['fsr1', 'fsr3'],\n",
    "    13: ['fsr2', 'fsr4'],\n",
    "    14: ['fsr2', 'fsr3'],\n",
    "\n",
    "}\n",
    "\n",
    "# Create new columns 'A' and 'B' to store the merged values\n",
    "big_dataframe['FSR1'] = ''\n",
    "big_dataframe['FSR2'] = ''\n",
    "\n",
    "# Copy columns within each group into new columns A and B\n",
    "for participant_id, cols_to_copy in participant_column_mapping.items():\n",
    "    mask = big_dataframe['participant'] == participant_id\n",
    "    big_dataframe.loc[mask, 'FSR1'] += big_dataframe.loc[mask, cols_to_copy[0]].astype(str) + ','\n",
    "    big_dataframe.loc[mask, 'FSR2'] += big_dataframe.loc[mask, cols_to_copy[1]].astype(str) + ','\n",
    "\n",
    "# Remove the trailing commas\n",
    "big_dataframe['FSR1'] = big_dataframe['FSR1'].str.rstrip(',')\n",
    "big_dataframe['FSR2'] = big_dataframe['FSR2'].str.rstrip(',')\n",
    "\n",
    "# Display the modified DataFrame\n",
    "#print(big_dataframe1)\n",
    "big_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad87f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataframe['FSR1'] = big_dataframe['FSR2'].astype(float)\n",
    "big_dataframe['FSR2'] = big_dataframe['FSR2'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataframe=big_dataframe.drop(columns=['fsr1','fsr2','fsr3','fsr4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382084",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_participants = [1, 2, 3, 4, 5, 6, 9, 12, 13]\n",
    "test_participants = [10, 11, 14]\n",
    "\n",
    "# Filter train_set and test_set based on 'participant' column\n",
    "train_set = big_dataframe[big_dataframe['participant'].isin(train_participants)].copy()\n",
    "test_set = big_dataframe[big_dataframe['participant'].isin(test_participants)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_serial_numbers = train_set['serialnumber'].unique()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate through unique serial numbers and append arrays and class values to the lists\n",
    "for serial_number in unique_serial_numbers:\n",
    "    subset_df = train_set[train_set['serialnumber'] == serial_number]\n",
    "    acc2x_y_array = subset_df[['gyro1','gyro2','gyro3']].to_numpy()\n",
    "    class_value = subset_df['class'].iloc[0]  # Get the first value of the 'class' column\n",
    "    X_train.append(acc2x_y_array)\n",
    "    y_train.append(class_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_serial_numbers = test_set['serialnumber'].unique()\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for serial_number in unique_serial_numbers:\n",
    "    subset_df = test_set[test_set['serialnumber'] == serial_number]\n",
    "    acc2x_y_array = subset_df[['gyro1','gyro2','gyro3']].to_numpy()\n",
    "    class_value = subset_df['class'].iloc[0]  # Get the first value of the 'class' column\n",
    "    X_test.append(acc2x_y_array)\n",
    "    y_test.append(class_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3039f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.utils import to_time_series_dataset\n",
    "X_train = to_time_series_dataset(X_train)\n",
    "X_test = to_time_series_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff845a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tslearn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da65af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(\"X_train size:\", len(X_train))\n",
    "print(\"X_test size:\", len(X_test))\n",
    "print(\"y_train size:\", len(y_train))\n",
    "print(\"y_test size:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn import metrics\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94650da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn dtw\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc_dtw = KNeighborsTimeSeriesClassifier(n_neighbors=2, metric=\"dtw\", n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "tsc_dtw.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400c9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cv\n",
    "logo = LeaveOneGroupOut()\n",
    "X_train_reshaped = np.array(X_train)\n",
    "accuracy_scores = cross_val_score(tsc_dtw, X_train_reshaped, y_train, groups=participant, cv=logo)\n",
    "\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "print(\"Average Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2085842",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_single_prediction = time.time()\n",
    "y_dtw = tsc_dtw.predict(X_test)\n",
    "single_prediction_time = time.time() - start_time_single_prediction\n",
    "print(\"Single Prediction time:\", single_prediction_time/len(y_test), \"seconds\")\n",
    " \n",
    "print(\"Accuracy using Knn: {0:.2f}\".format(accuracy_score(y_dtw,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report_svm = classification_report(y_test, y_dtw)\n",
    "print(\"Classification Report for SVM:\\n\", class_report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Default\",\"Stop\", \"Speed_Up\", \"Slow_Down\", \"Change_Direction\", \"Repeat\", \"Reset\"]\n",
    "\n",
    "label_mapping = {\n",
    "    0: \"Default\",\n",
    "    1: \"Stop\",\n",
    "    2: \"Speed_Up\",\n",
    "    3: \"Slow_Down\",\n",
    "    4: \"Change_Direction\",\n",
    "    5: \"Repeat\",\n",
    "    6: \"Reset\"\n",
    "}\n",
    "\n",
    "y_test_mapped = [label_mapping[label] for label in y_test]\n",
    "y_pred_mapped = [label_mapping[label] for label in y_dtw]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_mapped, y_pred_mapped, labels=class_labels)\n",
    "\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix with percentage\n",
    "conf_matrix = confusion_matrix(y_test_mapped, y_pred_mapped, labels=class_labels)\n",
    "\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "confusion_df_percent = confusion_df.divide(confusion_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Generate annotations with both actual number and percentage\n",
    "annotations = confusion_df.astype(str) + '\\n' + confusion_df_percent.round(2).astype(str) + '%'\n",
    "\n",
    "# Generate a heatmap with custom colors and annotation showing both number and percentage\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_df, annot=annotations, fmt='', cmap=\"Blues\", cbar_kws={'label': 'Counts'})\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "clf = TimeSeriesSVC(C=1.0, kernel=\"gak\")\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70435960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv-svm\n",
    "logo = LeaveOneGroupOut()\n",
    "clf = TimeSeriesSVC(C=1.0, kernel=\"gak\")\n",
    "X_train_reshaped = np.array(X_train)\n",
    "accuracy_scores = cross_val_score(clf, X_train_reshaped, y_train, groups=participant, cv=logo)\n",
    "\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "print(\"Average Accuracy:\", average_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13730c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_single_prediction = time.time()\n",
    "y_svm = clf.predict(X_test)#svm\n",
    "single_prediction_time = time.time() - start_time_single_prediction\n",
    "print(\"Single Prediction time:\", single_prediction_time/len(y_test), \"seconds\")\n",
    "\n",
    "print(\"Accuracy using SVM: {0:.2f}\".format(accuracy_score(y_svm,y_test)))#svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60928851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report_svm = classification_report(y_test, y_svm)\n",
    "\n",
    "print(\"Classification Report for SVM:\\n\", class_report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986dc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.svm import TimeSeriesSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid including bandwidth values\n",
    "param_grid = {\n",
    "    'C': [1,10,100.0],  # values of C to try\n",
    "    'gamma': [1,10,100.0],  # bandwidth values to try\n",
    "}\n",
    "\n",
    "svm_model = TimeSeriesSVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Default\",\"Stop\", \"Speed_Up\", \"Slow_Down\", \"Change_Direction\", \"Repeat\", \"Reset\"]\n",
    "# Define a mapping from numeric labels to class names\n",
    "label_mapping = {\n",
    "    0: \"Default\",\n",
    "    1: \"Stop\",\n",
    "    2: \"Speed_Up\",\n",
    "    3: \"Slow_Down\",\n",
    "    4: \"Change_Direction\",\n",
    "    5: \"Repeat\",\n",
    "    6: \"Reset\"\n",
    "}\n",
    "\n",
    "y_test_mapped = [label_mapping[label] for label in y_test]\n",
    "y_pred_mapped = [label_mapping[label] for label in y_svm]\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_mapped, y_pred_mapped, labels=class_labels)\n",
    "\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c05432",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test_mapped, y_pred_mapped, labels=class_labels)\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "confusion_df_percent = confusion_df.divide(confusion_df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Generate annotations with both actual number and percentage\n",
    "annotations = confusion_df.astype(str) + '\\n' + confusion_df_percent.round(2).astype(str) + '%'\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_df, annot=annotations, fmt='', cmap=\"Blues\", cbar_kws={'label': 'Counts'})\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce853be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
